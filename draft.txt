linear_model = LinearModel.create(no_features=4)
  .add_feature_column(col1)
  .add_feature_column(col2)
  .add_feature_column(col3)
  .add_feature_column(col4)
  .add_target_column(col5)
  .train()

output = lienar_model.predict(feature_vec)

==========================================================

Using Python Pandas, numpy, seaborn (and possibly scikit-learn):
- Load a CSV file called "loan_old.csv" (it has the following columns: Loan_ID,Gender,Married,Dependents,
  Education,Income,Coapplicant_Income,Loan_Tenor,Credit_History,Property_Area,Max_Loan_Amount,Loan_Status)
- Separate the columns called "Max_Loan_Amount" and "Loan_Status" (the target columns) from the rest of the columns
  (the feature columns)
- Check if there are records with missing values
  - If there are, remove them (and print a message)
- Shuffle the data and split it into training and testing sets
- Check the type of each feature and target column (categorical or numerical)
  - Print the type
  - If categorical feature or target column: label encode it
  - If numerical feature column: print its maximum, mean and minimum and standardize it
- Visualize the numerical feature columns as a seaborn pairplot
- If you need more context or details, let me know

==============================================================

- https://stackoverflow.com/questions/54906553/differentiating-between-numerical-and-categorical-columns
  - Credit history might be categorical

==============================================================

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# Load the CSV file
df = pd.read_csv("loan_old.csv")

# Separate the feature columns from the target columns
feature_cols = df.drop(columns=["Max_Loan_Amount", "Loan_Status"])
target_cols = df[["Max_Loan_Amount", "Loan_Status"]]

# Check for missing values
if feature_cols.isnull().sum().any() or target_cols.isnull().sum().any():
    feature_cols.dropna(inplace=True)
    target_cols.dropna(inplace=True)
    print("Records with missing values have been removed.")

# Shuffle the data and split it into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    feature_cols, target_cols, test_size=0.2, random_state=42)

# Check the type of each feature and target column
for col in feature_cols.columns:
    col_type = "categorical" if feature_cols[col].dtype == 'object' else "numerical"
    print(f"{col}: {col_type}")

    # If numerical feature column: print its maximum, mean and minimum
    if col_type == "numerical":
        print(f"  Maximum: {feature_cols[col].max()}")
        print(f"  Mean: {feature_cols[col].mean()}")
        print(f"  Minimum: {feature_cols[col].min()}")

    # Label encode categorical feature columns
    if col_type == "categorical":
        le = LabelEncoder()
        feature_cols[col] = le.fit_transform(feature_cols[col])

for col in target_cols.columns:
    col_type = "categorical" if target_cols[col].dtype == 'object' else "numerical"
    print(f"{col}: {col_type}")

    # Label encode categorical target columns
    if col_type == "categorical":
        le = LabelEncoder()
        target_cols[col] = le.fit_transform(target_cols[col])

# Standardize numerical feature columns
numerical_cols = feature_cols.select_dtypes(include=['float64', 'int64']).columns
scaler = StandardScaler()
feature_cols[numerical_cols] = scaler.fit_transform(feature_cols[numerical_cols])

# Visualize the numerical feature columns as a seaborn pairplot
sns.pairplot(feature_cols[numerical_cols])
