{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether there are missing values `b.i`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"loan_old.csv\")\n",
    "empty = df.isnull().sum().sum()\n",
    "print(\"There are \" + str(empty) + \" empty values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records containing missing values are removed `c.i`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    clean_df = df.drop(columns=[\"Loan_ID\"]) # No need for id as well\n",
    "    clean_df = clean_df.dropna()\n",
    "    return clean_df\n",
    "\n",
    "clean_df = preprocess_df(df)\n",
    "display(clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the type of each feature, and the scale of numerical features (implies separating the features and the targets)\n",
    "`b.ii`, `b.iii`, `c.ii`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = clean_df.drop(columns=[\"Max_Loan_Amount\", \"Loan_Status\"])\n",
    "targets_df = clean_df[[\"Max_Loan_Amount\", \"Loan_Status\"]]\n",
    "\n",
    "categorical_features_df = features_df.select_dtypes(include=[\"object\"])\n",
    "numerical_features_df = features_df.select_dtypes(exclude=[\"object\"])\n",
    "\n",
    "print(\"Categorical features:\")\n",
    "for col in categorical_features_df.columns:\n",
    "    print(f\"\\t- {col}\")\n",
    "print(\"Numerical features:\")\n",
    "for col in numerical_features_df.columns:\n",
    "    print(\n",
    "        f\"\\t- {col} ({numerical_features_df[col].min()} - {numerical_features_df[col].max()})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a pairplot between numerical columns `b.iv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(clean_df.select_dtypes(exclude=[\"object\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is shuffled and split into training and testing sets `c.iii`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "train_size = 1 - test_size\n",
    "\n",
    "(\n",
    "    features_train,\n",
    "    features_test,\n",
    "    max_loan_train,\n",
    "    max_loan_test,\n",
    "    loan_status_train,\n",
    "    loan_status_test,\n",
    ") = train_test_split(\n",
    "    features_df,\n",
    "    targets_df[\"Max_Loan_Amount\"],\n",
    "    targets_df[\"Loan_Status\"],\n",
    "    test_size=test_size,\n",
    "    train_size=train_size,\n",
    "    random_state=30,\n",
    ")\n",
    "\n",
    "print(\"Features training set\")\n",
    "display(features_train)\n",
    "print(\"Features testing set\")\n",
    "display(features_test)\n",
    "print(\"Max loan (target) training\")\n",
    "display(pd.DataFrame(max_loan_train))\n",
    "print(\"Max loan (target) testing\")\n",
    "display(pd.DataFrame(max_loan_test))\n",
    "print(\"Loan status (target) training\")\n",
    "display(pd.DataFrame(loan_status_train))\n",
    "print(\"Loan status (target) testing\")\n",
    "display(pd.DataFrame(loan_status_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:\n",
    "- Categorical features are encoded `c.iv`\n",
    "- Numerical features are standardized `c.vi`\n",
    "- Categorical targets are encoded `c.v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "standard_scalers = {}\n",
    "processed_features_train = pd.DataFrame(index=features_train.index)\n",
    "\n",
    "for col in features_train.columns:\n",
    "    if features_train[col].dtype == \"object\":\n",
    "        print(f\"Encoding {col}\")\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "        processed_features_train[col] = label_encoders[col].fit_transform(features_train[col])\n",
    "        print(f\"\\t- Before: {label_encoders[col].classes_}\")\n",
    "        print(f\"\\t- After: {np.unique(processed_features_train[col])}\")\n",
    "    else:\n",
    "        print(f\"Standardizing {col}\")\n",
    "        print(\n",
    "            f\"\\t- Before: {np.min(features_train[col])} to {np.max(features_train[col])}\"\n",
    "        )\n",
    "        standard_scalers[col] = StandardScaler()\n",
    "        processed_features_train[col] = standard_scalers[col].fit_transform(features_train[[col]])\n",
    "        print(\n",
    "            f\"\\t- After: {np.min(processed_features_train[col])} to {np.max(processed_features_train[col])}\"\n",
    "        )\n",
    "\n",
    "display(features_train)\n",
    "display(processed_features_train)\n",
    "print(f\"Encoding training Loan_Status\")\n",
    "loan_status_encoder = LabelEncoder()\n",
    "processed_loan_status_train = pd.Series(\n",
    "    loan_status_encoder.fit_transform(loan_status_train),\n",
    "    name=loan_status_train.name,\n",
    ")\n",
    "print(f\"\\t- Before: {loan_status_encoder.classes_}\")\n",
    "print(f\"\\t- After: {np.unique(processed_loan_status_train)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear regression model to the data to predict the loan amount. `d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(processed_features_train, max_loan_train)\n",
    "print(linear_model.feature_names_in_)\n",
    "print(linear_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the linear regression model using sklearn's R2 score. `e`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_new_features(label_encoders, standard_scalers, new_features_df):\n",
    "    processed_new_features_df = pd.DataFrame(index=new_features_df.index)\n",
    "    for col in new_features_df.columns:\n",
    "        if new_features_df[col].dtype == \"object\":\n",
    "            print(f\"Encoding {col}\")\n",
    "            processed_new_features_df[col] = label_encoders[col].transform(\n",
    "                new_features_df[col]\n",
    "            )\n",
    "            print(f\"\\t- Before: {label_encoders[col].classes_}\")\n",
    "            print(f\"\\t- After: {np.unique(processed_new_features_df[col])}\")\n",
    "        else:\n",
    "            print(f\"Standardizing {col}\")\n",
    "            print(\n",
    "                f\"\\t- Before: {np.min(new_features_df[col])} to {np.max(new_features_df[col])}\"\n",
    "            )\n",
    "            processed_new_features_df[col] = standard_scalers[col].transform(\n",
    "                new_features_df[[col]]\n",
    "            )\n",
    "            print(\n",
    "                f\"\\t- After: {np.min(processed_new_features_df[col])} to {np.max(processed_new_features_df[col])}\"\n",
    "            )\n",
    "    return processed_new_features_df\n",
    "\n",
    "\n",
    "processed_features_test = preprocess_new_features(\n",
    "    label_encoders, standard_scalers, features_test\n",
    ")\n",
    "print(f\"R^2 score: {linear_model.score(processed_features_test, max_loan_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a logistic regression model to the data to predict the loan status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def lg_gradient_descent(learning_rate, epochs, X, y):\n",
    "    X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros(n)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        z = np.dot(X, weights)\n",
    "        h = sigmoid(z)\n",
    "        diff = h - y\n",
    "\n",
    "        gradients = np.dot(X.T, diff) / m\n",
    "\n",
    "        weights -= learning_rate * gradients\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def lg_predict(weights, X):\n",
    "    X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = sigmoid(z)\n",
    "    return (predictions > 0.5).astype(int)\n",
    "\n",
    "\n",
    "X_lg_train = processed_features_train.values\n",
    "y_lg_train = processed_loan_status_train.values\n",
    "lg_learning_rate = 0.01\n",
    "lg_epochs = 500\n",
    "lg_weights = lg_gradient_descent(lg_learning_rate, lg_epochs, X_lg_train, y_lg_train)\n",
    "print(lg_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function (from scratch) to calculate the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_loan_status_test = pd.Series(\n",
    "    loan_status_encoder.transform(loan_status_test),\n",
    "    name=loan_status_test.name,\n",
    ")\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = np.sum(y_pred == y_true)\n",
    "    total_predictions = len(y_true)\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "y_true = processed_loan_status_test.values\n",
    "y_pred = lg_predict(lg_weights, processed_features_test)\n",
    "print(f\"Accuracy: {calculate_accuracy(y_true, y_pred) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the \"loan_new.csv\" dataset, perform the same preprocessing on it (except shuffling and splitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"loan_new.csv\")\n",
    "new_df_processed = preprocess_df(new_df)\n",
    "new_features_processed = preprocess_new_features(label_encoders, standard_scalers, new_df_processed)\n",
    "new_maximum_loan_amounts = linear_model.predict(new_features_processed)\n",
    "new_loan_statuses = lg_predict(lg_weights, new_features_processed)\n",
    "\n",
    "new_df_processed[\"Max_Loan_Amount\"] = new_maximum_loan_amounts\n",
    "new_df_processed[\"Loan_Status\"] = loan_status_encoder.inverse_transform(new_loan_statuses)\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(new_df_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
